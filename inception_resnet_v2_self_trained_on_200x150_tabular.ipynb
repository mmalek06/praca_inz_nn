{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "The previous notebook (inception_resnet_v2_self_trained_on_200x150) created \"baseline models\" that I can improve on. For example I haven’t used the tabular data and neither have I used any image augmentation. Also, the original architecture likes square images, but I’m giving it rectangular ones. ROI detection net will have to be trained to enable the possibility to detect the main location of the lesion and crop the image accordingly. Both of these points will be addressed next, and this notebook is about improving the results by using tabular data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import absl.logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "get_names = lambda root_path: [\n",
    "    file_name.split('.')[0]\n",
    "    for dir_path, _, file_names in os.walk(root_path)\n",
    "    for file_name in file_names\n",
    "]\n",
    "base_dir = os.path.join('..', 'data', 'images_original_inception_resnet_v2_200x150_splitted')\n",
    "train_dir = os.path.join(base_dir, 'training')\n",
    "valid_dir = os.path.join(base_dir, 'validation')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating the tabular data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def normalize(data: np.ndarray) -> np.ndarray:\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "\n",
    "metadata_path = os.path.join('..', 'data', 'HAM10000_metadata.csv')\n",
    "data = pd.read_csv(metadata_path)\n",
    "data['age'] = data.groupby('sex')['age'].transform(lambda x: x.fillna(x.mean()))\n",
    "train_names = set(get_names(train_dir))\n",
    "valid_names = set(get_names(valid_dir))\n",
    "relevant_cols = ['dx', 'age', 'sex', 'localization']\n",
    "train_df = data[data['image_id'].isin(train_names)].sort_values(by='image_id')[relevant_cols]\n",
    "valid_df = data[data['image_id'].isin(valid_names)].sort_values(by='image_id')[relevant_cols]\n",
    "train_age = np.expand_dims(normalize(train_df['age'].to_numpy()), -1)\n",
    "train_sex_categories = pd.get_dummies(train_df['sex']).to_numpy()\n",
    "train_localization_categories = pd.get_dummies(train_df['localization']).to_numpy()\n",
    "valid_age = np.expand_dims(normalize(valid_df['age'].to_numpy()), -1)\n",
    "valid_sex_categories = pd.get_dummies(valid_df['sex']).to_numpy()\n",
    "valid_localization_categories = pd.get_dummies(valid_df['localization']).to_numpy()\n",
    "X_train = np.hstack((train_age, train_sex_categories, train_localization_categories))\n",
    "y_train = pd.get_dummies(train_df['dx'])\n",
    "X_valid = np.hstack((valid_age, valid_sex_categories, valid_localization_categories))\n",
    "y_valid = pd.get_dummies(valid_df['dx'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating the image data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8015 files belonging to 1 classes.\n",
      "Found 2000 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "def preprocess_dataset(dataset: tf.data.Dataset) -> tf.data.Dataset:\n",
    "    rescale = keras.layers.Rescaling(1./255)\n",
    "\n",
    "    return dataset.map(lambda image: rescale(image))\n",
    "\n",
    "\n",
    "train_generator = keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels=None,\n",
    "    label_mode=None,\n",
    "    image_size=(150, 200),\n",
    "    batch_size=64,\n",
    "    shuffle=False) # setting this param to None because the labels will be taken care of separately\n",
    "valid_generator = keras.utils.image_dataset_from_directory(\n",
    "    valid_dir,\n",
    "    labels=None,\n",
    "    label_mode=None,\n",
    "    image_size=(150, 200),\n",
    "    batch_size=64,\n",
    "    shuffle=False) # setting this param to None because the labels will be taken care of separately\n",
    "train_generator = preprocess_dataset(train_generator)\n",
    "valid_generator = preprocess_dataset(valid_generator)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "train_metadata_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(64)\n",
    "valid_metadata_ds = tf.data.Dataset.from_tensor_slices((X_valid, y_valid)).batch(64)\n",
    "train_dataset = tf.data.Dataset.zip((train_generator, train_metadata_ds))\n",
    "valid_dataset = tf.data.Dataset.zip((valid_generator, valid_metadata_ds))\n",
    "\n",
    "\n",
    "def restructure(image_data, tabular_data_and_labels):\n",
    "    tabular_data, labels = tabular_data_and_labels\n",
    "\n",
    "    return (image_data, tabular_data), labels\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(restructure)\n",
    "valid_dataset = valid_dataset.map(restructure)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "SMALLER_WIDTH = 600 // 3\n",
    "SMALLER_HEIGHT = 450 // 3\n",
    "METADATA_COLS_COUNT = X_train.shape[1]\n",
    "\n",
    "\n",
    "def get_model() -> keras.Model:\n",
    "    base_model = InceptionResNetV2(include_top=False, weights=None, input_shape=(SMALLER_HEIGHT, SMALLER_WIDTH, 3))\n",
    "    img_x = base_model.output\n",
    "    img_x = keras.layers.Dropout(.4)(img_x)\n",
    "    img_x = keras.layers.GlobalAveragePooling2D()(img_x)\n",
    "    img_x = keras.layers.Dense(512)(img_x)\n",
    "    img_x = keras.layers.PReLU()(img_x)\n",
    "    img_x = keras.layers.Dropout(.4)(img_x)\n",
    "    img_x = keras.layers.Dense(512)(img_x)\n",
    "    img_x = keras.layers.PReLU()(img_x)\n",
    "    tab_input = keras.Input(shape=(METADATA_COLS_COUNT,), name='tab_input')\n",
    "    tab_x = keras.layers.Dense(128)(tab_input)\n",
    "    tab_x = keras.layers.PReLU()(tab_x)\n",
    "    tab_x = keras.layers.Dense(128)(tab_x)\n",
    "    tab_x = keras.layers.PReLU()(tab_x)\n",
    "    img_x = keras.layers.Dropout(.4)(img_x)\n",
    "    combined = keras.layers.concatenate([img_x, tab_x])\n",
    "    predictions = keras.layers.Dense(7, activation='softmax')(combined)\n",
    "    model = keras.Model(inputs=[base_model.input, tab_input], outputs=predictions)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def run_model(model_factory, model_name: str) -> None:\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=10,\n",
    "                                                   min_delta=1e-6)\n",
    "    model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath='models/' + model_name + '_{epoch}',\n",
    "        save_best_only=True)\n",
    "    tensor_board = keras.callbacks.TensorBoard(log_dir=f'tensor_logs/{model_name}')\n",
    "    model = model_factory()\n",
    "\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=valid_dataset,\n",
    "        epochs=50,\n",
    "        callbacks=[early_stopping, model_checkpoint, tensor_board])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9338 - accuracy: 0.6841INFO:tensorflow:Assets written to: models\\images_original_inception_resnet_v2_200x150_with_tabular3_1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\images_original_inception_resnet_v2_200x150_with_tabular3_1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/126 [==============================] - 136s 953ms/step - loss: 0.9338 - accuracy: 0.6841 - val_loss: 1.1889 - val_accuracy: 0.6520\n",
      "Epoch 2/50\n",
      "126/126 [==============================] - 54s 426ms/step - loss: 0.7473 - accuracy: 0.7275 - val_loss: 1.3337 - val_accuracy: 0.5440\n",
      "Epoch 3/50\n",
      "126/126 [==============================] - 54s 427ms/step - loss: 0.7196 - accuracy: 0.7351 - val_loss: 1.3653 - val_accuracy: 0.4760\n",
      "Epoch 4/50\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.7035 - accuracy: 0.7384INFO:tensorflow:Assets written to: models\\images_original_inception_resnet_v2_200x150_with_tabular3_4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\images_original_inception_resnet_v2_200x150_with_tabular3_4\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/126 [==============================] - 115s 915ms/step - loss: 0.7035 - accuracy: 0.7384 - val_loss: 1.0003 - val_accuracy: 0.7185\n",
      "Epoch 5/50\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.6723 - accuracy: 0.7513INFO:tensorflow:Assets written to: models\\images_original_inception_resnet_v2_200x150_with_tabular3_5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\images_original_inception_resnet_v2_200x150_with_tabular3_5\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/126 [==============================] - 115s 913ms/step - loss: 0.6723 - accuracy: 0.7513 - val_loss: 0.9436 - val_accuracy: 0.6445\n",
      "Epoch 6/50\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.6524 - accuracy: 0.7581INFO:tensorflow:Assets written to: models\\images_original_inception_resnet_v2_200x150_with_tabular3_6\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\images_original_inception_resnet_v2_200x150_with_tabular3_6\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/126 [==============================] - 113s 899ms/step - loss: 0.6524 - accuracy: 0.7581 - val_loss: 0.7924 - val_accuracy: 0.7225\n",
      "Epoch 7/50\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.6314 - accuracy: 0.7653INFO:tensorflow:Assets written to: models\\images_original_inception_resnet_v2_200x150_with_tabular3_7\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\images_original_inception_resnet_v2_200x150_with_tabular3_7\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/126 [==============================] - 115s 916ms/step - loss: 0.6314 - accuracy: 0.7653 - val_loss: 0.7645 - val_accuracy: 0.7065\n",
      "Epoch 8/50\n",
      "126/126 [==============================] - 54s 429ms/step - loss: 0.6195 - accuracy: 0.7668 - val_loss: 0.7793 - val_accuracy: 0.6995\n",
      "Epoch 9/50\n",
      "126/126 [==============================] - 54s 426ms/step - loss: 0.6053 - accuracy: 0.7747 - val_loss: 0.7864 - val_accuracy: 0.7240\n",
      "Epoch 10/50\n",
      "126/126 [==============================] - 54s 425ms/step - loss: 0.5874 - accuracy: 0.7842 - val_loss: 0.8389 - val_accuracy: 0.7040\n",
      "Epoch 11/50\n",
      "126/126 [==============================] - 54s 426ms/step - loss: 0.5783 - accuracy: 0.7840 - val_loss: 0.9180 - val_accuracy: 0.6930\n",
      "Epoch 12/50\n",
      "126/126 [==============================] - 54s 426ms/step - loss: 0.5692 - accuracy: 0.7860 - val_loss: 0.9417 - val_accuracy: 0.7045\n",
      "Epoch 13/50\n",
      "126/126 [==============================] - 54s 427ms/step - loss: 0.5707 - accuracy: 0.7904 - val_loss: 0.9243 - val_accuracy: 0.7250\n",
      "Epoch 14/50\n",
      "126/126 [==============================] - 54s 429ms/step - loss: 0.5385 - accuracy: 0.8002 - val_loss: 0.8871 - val_accuracy: 0.7330\n",
      "Epoch 15/50\n",
      "126/126 [==============================] - 54s 426ms/step - loss: 0.5241 - accuracy: 0.8062 - val_loss: 0.9561 - val_accuracy: 0.7155\n",
      "Epoch 16/50\n",
      "126/126 [==============================] - 54s 426ms/step - loss: 0.4997 - accuracy: 0.8150 - val_loss: 0.8667 - val_accuracy: 0.7175\n",
      "Epoch 17/50\n",
      "126/126 [==============================] - 54s 426ms/step - loss: 0.4876 - accuracy: 0.8180 - val_loss: 1.0112 - val_accuracy: 0.7030\n",
      "Epoch 18/50\n",
      "126/126 [==============================] - 54s 426ms/step - loss: 0.4704 - accuracy: 0.8263 - val_loss: 0.8503 - val_accuracy: 0.7160\n",
      "Epoch 19/50\n",
      "126/126 [==============================] - 54s 425ms/step - loss: 0.5078 - accuracy: 0.8136 - val_loss: 0.9978 - val_accuracy: 0.7075\n",
      "Epoch 20/50\n",
      "126/126 [==============================] - 54s 425ms/step - loss: 0.4517 - accuracy: 0.8339 - val_loss: 0.9739 - val_accuracy: 0.6985\n",
      "Epoch 21/50\n",
      "126/126 [==============================] - 54s 427ms/step - loss: 0.4342 - accuracy: 0.8409 - val_loss: 0.8023 - val_accuracy: 0.7040\n",
      "Epoch 22/50\n",
      "126/126 [==============================] - 54s 429ms/step - loss: 0.4925 - accuracy: 0.8222 - val_loss: 0.9739 - val_accuracy: 0.6955\n",
      "Epoch 23/50\n",
      "126/126 [==============================] - 54s 428ms/step - loss: 0.4010 - accuracy: 0.8550 - val_loss: 0.8900 - val_accuracy: 0.7240\n",
      "Epoch 24/50\n",
      "126/126 [==============================] - 54s 427ms/step - loss: 0.3524 - accuracy: 0.8702 - val_loss: 0.7824 - val_accuracy: 0.7330\n"
     ]
    }
   ],
   "source": [
    "run_model(get_model, f'images_original_inception_resnet_v2_200x150_with_tabular3')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And the final verdict is that using tabular data makes the network do worse than it would without it. It may have a potentially logical explanation - the tabular data is really simple and consists of age, sex and lesion location. Each of those even combined with each other is a weak indicator for skin cancer and some additional tabular data would have to be used to actually improve the CNN result."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
