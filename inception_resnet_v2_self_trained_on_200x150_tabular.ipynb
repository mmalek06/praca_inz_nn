{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "The previous notebook (inception_resnet_v2_self_trained_on_200x150) created \"baseline models\" that I can improve on. For example I haven’t used the tabular data and neither have I used any image augmentation. Also, the original architecture likes square images, but I’m giving it rectangular ones. ROI detection net will have to be trained to enable the possibility to detect the main location of the lesion and crop the image accordingly. Both of these points will be addressed next, and this notebook is about improving the results by using tabular data."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import os\n",
    "import absl.logging\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "get_names = lambda root_path: [\n",
    "    file_name.split('.')[0]\n",
    "    for dir_path, _, file_names in os.walk(root_path)\n",
    "    for file_name in file_names\n",
    "]\n",
    "base_dir = os.path.join('..', 'data', 'images_original_inception_resnet_v2_200x150_splitted')\n",
    "train_dir = os.path.join(base_dir, 'training')\n",
    "valid_dir = os.path.join(base_dir, 'validation')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating the tabular data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "metadata_path = os.path.join('..', 'data', 'HAM10000_metadata.csv')\n",
    "data = pd.read_csv(metadata_path)\n",
    "data['age'] = data.groupby('sex')['age'].transform(lambda x: x.fillna(x.mean()))\n",
    "train_names = set(get_names(train_dir))\n",
    "valid_names = set(get_names(valid_dir))\n",
    "relevant_cols = ['dx', 'age', 'sex', 'localization']\n",
    "train_df = data[data['image_id'].isin(train_names)].sort_values(by='image_id')[relevant_cols]\n",
    "valid_df = data[data['image_id'].isin(valid_names)].sort_values(by='image_id')[relevant_cols]\n",
    "train_age = np.expand_dims(train_df['age'].astype(int).to_numpy(), -1)\n",
    "train_sex_categories = pd.get_dummies(train_df['sex']).to_numpy()\n",
    "train_localization_categories = pd.get_dummies(train_df['localization']).to_numpy()\n",
    "valid_age = np.expand_dims(valid_df['age'].astype(int).to_numpy(), -1)\n",
    "valid_sex_categories = pd.get_dummies(valid_df['sex']).to_numpy()\n",
    "valid_localization_categories = pd.get_dummies(valid_df['localization']).to_numpy()\n",
    "X_train = np.hstack((train_age, train_sex_categories, train_localization_categories))\n",
    "y_train = pd.get_dummies(train_df['dx'])\n",
    "X_valid = np.hstack((valid_age, valid_sex_categories, valid_localization_categories))\n",
    "y_valid = pd.get_dummies(valid_df['dx'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creating the image data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8015 files belonging to 1 classes.\n",
      "Found 2000 files belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "def preprocess_dataset(dataset: tf.data.Dataset) -> tf.data.Dataset:\n",
    "    rescale = keras.layers.Rescaling(1./255)\n",
    "\n",
    "    return dataset.map(lambda image: rescale(image))\n",
    "\n",
    "\n",
    "train_generator = keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    labels=None,\n",
    "    label_mode=None,\n",
    "    image_size=(150, 200),\n",
    "    batch_size=64,\n",
    "    shuffle=False) # setting this param to None because the labels will be taken care of separately\n",
    "valid_generator = keras.utils.image_dataset_from_directory(\n",
    "    valid_dir,\n",
    "    labels=None,\n",
    "    label_mode=None,\n",
    "    image_size=(150, 200),\n",
    "    batch_size=64,\n",
    "    shuffle=False) # setting this param to None because the labels will be taken care of separately\n",
    "train_generator = preprocess_dataset(train_generator)\n",
    "valid_generator = preprocess_dataset(valid_generator)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "train_metadata_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(64)\n",
    "valid_metadata_ds = tf.data.Dataset.from_tensor_slices((X_valid, y_valid)).batch(64)\n",
    "train_dataset = tf.data.Dataset.zip((train_generator, train_metadata_ds))\n",
    "valid_dataset = tf.data.Dataset.zip((valid_generator, valid_metadata_ds))\n",
    "\n",
    "\n",
    "def restructure(image_data, tabular_data_and_labels):\n",
    "    tabular_data, labels = tabular_data_and_labels\n",
    "\n",
    "    return (image_data, tabular_data), labels\n",
    "\n",
    "\n",
    "train_dataset = train_dataset.map(restructure)\n",
    "valid_dataset = valid_dataset.map(restructure)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "SMALLER_WIDTH = 600 // 3\n",
    "SMALLER_HEIGHT = 450 // 3\n",
    "METADATA_COLS_COUNT = X_train.shape[1]\n",
    "\n",
    "\n",
    "def get_model() -> keras.Model:\n",
    "    base_model = InceptionResNetV2(include_top=False, weights=None, input_shape=(SMALLER_HEIGHT, SMALLER_WIDTH, 3))\n",
    "    img_x = base_model.output\n",
    "    img_x = keras.layers.Dropout(.4)(img_x)\n",
    "    img_x = keras.layers.GlobalAveragePooling2D()(img_x)\n",
    "    img_x = keras.layers.Dense(512)(img_x)\n",
    "    img_x = keras.layers.PReLU()(img_x)\n",
    "    img_x = keras.layers.Dropout(.4)(img_x)\n",
    "    img_x = keras.layers.Dense(512)(img_x)\n",
    "    img_x = keras.layers.PReLU()(img_x)\n",
    "    tab_input = keras.Input(shape=(METADATA_COLS_COUNT,), name='tab_input')\n",
    "    tab_x = keras.layers.Dense(64)(tab_input)\n",
    "    tab_x = keras.layers.PReLU()(tab_x)\n",
    "    tab_x = keras.layers.Dense(64)(tab_x)\n",
    "    tab_x = keras.layers.PReLU()(tab_x)\n",
    "    img_x = keras.layers.Dropout(.2)(img_x)\n",
    "    combined = keras.layers.concatenate([img_x, tab_x])\n",
    "    predictions = keras.layers.Dense(7, activation='softmax')(combined)\n",
    "    model = keras.Model(inputs=[base_model.input, tab_input], outputs=predictions)\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "def run_model(model_factory, model_name: str) -> None:\n",
    "    early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10,\n",
    "                                                   min_delta=1e-6)\n",
    "    model_checkpoint = keras.callbacks.ModelCheckpoint(\n",
    "        filepath='models/' + model_name + '{epoch}',\n",
    "        save_best_only=True)\n",
    "    tensor_board = keras.callbacks.TensorBoard(log_dir=f'tensor_logs/{model_name}')\n",
    "    model = model_factory()\n",
    "\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=valid_dataset,\n",
    "        epochs=50,\n",
    "        callbacks=[early_stopping, model_checkpoint, tensor_board])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.9454 - accuracy: 0.6812INFO:tensorflow:Assets written to: models\\images_original_inception_resnet_v2_200x150_with_tabular1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\images_original_inception_resnet_v2_200x150_with_tabular1\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/126 [==============================] - 136s 913ms/step - loss: 0.9454 - accuracy: 0.6812 - val_loss: 1.1018 - val_accuracy: 0.6705\n",
      "Epoch 2/50\n",
      "126/126 [==============================] - 52s 409ms/step - loss: 0.8429 - accuracy: 0.7034 - val_loss: 1.3326 - val_accuracy: 0.5660\n",
      "Epoch 3/50\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.7862 - accuracy: 0.7164INFO:tensorflow:Assets written to: models\\images_original_inception_resnet_v2_200x150_with_tabular3\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\images_original_inception_resnet_v2_200x150_with_tabular3\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/126 [==============================] - 112s 890ms/step - loss: 0.7862 - accuracy: 0.7164 - val_loss: 1.0806 - val_accuracy: 0.6430\n",
      "Epoch 4/50\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.7238 - accuracy: 0.7341INFO:tensorflow:Assets written to: models\\images_original_inception_resnet_v2_200x150_with_tabular4\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\images_original_inception_resnet_v2_200x150_with_tabular4\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/126 [==============================] - 112s 894ms/step - loss: 0.7238 - accuracy: 0.7341 - val_loss: 1.0375 - val_accuracy: 0.6255\n",
      "Epoch 5/50\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.7066 - accuracy: 0.7399INFO:tensorflow:Assets written to: models\\images_original_inception_resnet_v2_200x150_with_tabular5\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\images_original_inception_resnet_v2_200x150_with_tabular5\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/126 [==============================] - 112s 893ms/step - loss: 0.7066 - accuracy: 0.7399 - val_loss: 0.9236 - val_accuracy: 0.7105\n",
      "Epoch 6/50\n",
      "126/126 [==============================] - 52s 412ms/step - loss: 0.6764 - accuracy: 0.7563 - val_loss: 1.0962 - val_accuracy: 0.6245\n",
      "Epoch 7/50\n",
      "126/126 [==============================] - 52s 413ms/step - loss: 0.6657 - accuracy: 0.7577 - val_loss: 0.9311 - val_accuracy: 0.6710\n",
      "Epoch 8/50\n",
      "126/126 [==============================] - 52s 414ms/step - loss: 0.6335 - accuracy: 0.7649 - val_loss: 0.9856 - val_accuracy: 0.6155\n",
      "Epoch 9/50\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.6193 - accuracy: 0.7716INFO:tensorflow:Assets written to: models\\images_original_inception_resnet_v2_200x150_with_tabular9\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\images_original_inception_resnet_v2_200x150_with_tabular9\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/126 [==============================] - 113s 901ms/step - loss: 0.6193 - accuracy: 0.7716 - val_loss: 0.8421 - val_accuracy: 0.6795\n",
      "Epoch 10/50\n",
      "126/126 [==============================] - 52s 409ms/step - loss: 0.6143 - accuracy: 0.7716 - val_loss: 0.9991 - val_accuracy: 0.6225\n",
      "Epoch 11/50\n",
      "126/126 [==============================] - 52s 411ms/step - loss: 0.6047 - accuracy: 0.7798 - val_loss: 0.8496 - val_accuracy: 0.7260\n",
      "Epoch 12/50\n",
      "126/126 [==============================] - 52s 412ms/step - loss: 0.5736 - accuracy: 0.7853 - val_loss: 1.4529 - val_accuracy: 0.4190\n",
      "Epoch 13/50\n",
      "126/126 [==============================] - 52s 411ms/step - loss: 0.5747 - accuracy: 0.7925 - val_loss: 0.9762 - val_accuracy: 0.6130\n",
      "Epoch 14/50\n",
      "126/126 [==============================] - 52s 411ms/step - loss: 0.5562 - accuracy: 0.8002 - val_loss: 1.5295 - val_accuracy: 0.4965\n",
      "Epoch 15/50\n",
      "126/126 [==============================] - 52s 416ms/step - loss: 0.5899 - accuracy: 0.7814 - val_loss: 0.9193 - val_accuracy: 0.7225\n",
      "Epoch 16/50\n",
      "126/126 [==============================] - 52s 411ms/step - loss: 0.5328 - accuracy: 0.8026 - val_loss: 1.0287 - val_accuracy: 0.7120\n",
      "Epoch 17/50\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.5347 - accuracy: 0.8054INFO:tensorflow:Assets written to: models\\images_original_inception_resnet_v2_200x150_with_tabular17\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\images_original_inception_resnet_v2_200x150_with_tabular17\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/126 [==============================] - 112s 889ms/step - loss: 0.5347 - accuracy: 0.8054 - val_loss: 0.8123 - val_accuracy: 0.7060\n",
      "Epoch 18/50\n",
      "126/126 [==============================] - 52s 412ms/step - loss: 0.4911 - accuracy: 0.8198 - val_loss: 0.9034 - val_accuracy: 0.6990\n",
      "Epoch 19/50\n",
      "126/126 [==============================] - 52s 413ms/step - loss: 0.4695 - accuracy: 0.8235 - val_loss: 0.8829 - val_accuracy: 0.7355\n",
      "Epoch 20/50\n",
      "126/126 [==============================] - 52s 414ms/step - loss: 0.4263 - accuracy: 0.8443 - val_loss: 1.0627 - val_accuracy: 0.7010\n",
      "Epoch 21/50\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.4117 - accuracy: 0.8507INFO:tensorflow:Assets written to: models\\images_original_inception_resnet_v2_200x150_with_tabular21\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\images_original_inception_resnet_v2_200x150_with_tabular21\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/126 [==============================] - 112s 894ms/step - loss: 0.4117 - accuracy: 0.8507 - val_loss: 0.7740 - val_accuracy: 0.7240\n",
      "Epoch 22/50\n",
      "126/126 [==============================] - 53s 416ms/step - loss: 0.3663 - accuracy: 0.8675 - val_loss: 1.0181 - val_accuracy: 0.7495\n",
      "Epoch 23/50\n",
      "126/126 [==============================] - 53s 418ms/step - loss: 0.3353 - accuracy: 0.8774 - val_loss: 1.0516 - val_accuracy: 0.7130\n",
      "Epoch 24/50\n",
      "126/126 [==============================] - 52s 414ms/step - loss: 0.3640 - accuracy: 0.8722 - val_loss: 1.2376 - val_accuracy: 0.6475\n",
      "Epoch 25/50\n",
      "126/126 [==============================] - 52s 414ms/step - loss: 0.3644 - accuracy: 0.8715 - val_loss: 1.3661 - val_accuracy: 0.7650\n",
      "Epoch 26/50\n",
      "126/126 [==============================] - 52s 416ms/step - loss: 0.5504 - accuracy: 0.8056 - val_loss: 23.7458 - val_accuracy: 0.1775\n",
      "Epoch 27/50\n",
      "126/126 [==============================] - ETA: 0s - loss: 0.4747 - accuracy: 0.8312INFO:tensorflow:Assets written to: models\\images_original_inception_resnet_v2_200x150_with_tabular27\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models\\images_original_inception_resnet_v2_200x150_with_tabular27\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/126 [==============================] - 112s 895ms/step - loss: 0.4747 - accuracy: 0.8312 - val_loss: 0.7438 - val_accuracy: 0.7700\n",
      "Epoch 28/50\n",
      "126/126 [==============================] - 53s 418ms/step - loss: 0.3709 - accuracy: 0.8669 - val_loss: 1.1455 - val_accuracy: 0.7130\n",
      "Epoch 29/50\n",
      "126/126 [==============================] - 52s 414ms/step - loss: 0.3472 - accuracy: 0.8781 - val_loss: 1.1775 - val_accuracy: 0.7005\n",
      "Epoch 30/50\n",
      "126/126 [==============================] - 52s 414ms/step - loss: 0.2965 - accuracy: 0.8972 - val_loss: 1.0449 - val_accuracy: 0.7175\n",
      "Epoch 31/50\n",
      "126/126 [==============================] - 52s 415ms/step - loss: 0.3311 - accuracy: 0.8837 - val_loss: 1.0015 - val_accuracy: 0.6840\n",
      "Epoch 32/50\n",
      "126/126 [==============================] - 52s 415ms/step - loss: 0.2743 - accuracy: 0.9023 - val_loss: 1.2675 - val_accuracy: 0.6735\n",
      "Epoch 33/50\n",
      "126/126 [==============================] - 52s 415ms/step - loss: 0.1982 - accuracy: 0.9308 - val_loss: 1.0947 - val_accuracy: 0.7270\n",
      "Epoch 34/50\n",
      "126/126 [==============================] - 52s 415ms/step - loss: 0.1495 - accuracy: 0.9487 - val_loss: 1.5165 - val_accuracy: 0.6405\n",
      "Epoch 35/50\n",
      "126/126 [==============================] - 52s 412ms/step - loss: 0.1466 - accuracy: 0.9505 - val_loss: 1.6282 - val_accuracy: 0.6915\n",
      "Epoch 36/50\n",
      "126/126 [==============================] - 52s 413ms/step - loss: 0.1153 - accuracy: 0.9626 - val_loss: 1.4848 - val_accuracy: 0.7420\n",
      "Epoch 37/50\n",
      "126/126 [==============================] - 52s 413ms/step - loss: 0.1455 - accuracy: 0.9528 - val_loss: 1.3684 - val_accuracy: 0.7305\n"
     ]
    }
   ],
   "source": [
    "run_model(get_model, f'images_original_inception_resnet_v2_200x150_with_tabular')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
