{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_simplest_model2() -> keras.Model:\n",
    "    def get_attention_module(prev: keras.layers.Layer) -> keras.layers.Layer:\n",
    "        gap_layer = keras.layers.GlobalAveragePooling2D()(prev)\n",
    "        gap_layer_res = keras.layers.Reshape((1, 1, 256))(gap_layer)\n",
    "        dense = keras.layers.Dense(256, activation='relu')(gap_layer_res)\n",
    "        dense = keras.layers.Dense(256, activation='softmax')(dense)\n",
    "        mul_layer = keras.layers.Multiply()([prev, dense])\n",
    "\n",
    "        return mul_layer\n",
    "\n",
    "    def get_conv_module(\n",
    "            prev: keras.layers.Layer,\n",
    "            filters: int,\n",
    "            drop_rate: float,\n",
    "            kernel_size: int) -> keras.layers.Layer:\n",
    "        module = keras.layers.Conv2D(filters, kernel_size, strides=2, activation='relu', padding='same')(prev)\n",
    "        module = keras.layers.BatchNormalization()(module)\n",
    "        module = keras.layers.Conv2D(filters, kernel_size, activation='relu', padding='same')(module)\n",
    "        module = keras.layers.BatchNormalization()(module)\n",
    "        module = keras.layers.MaxPool2D(pool_size=2)(module)\n",
    "        module = keras.layers.Dropout(drop_rate)(module)\n",
    "\n",
    "        return module\n",
    "\n",
    "    def get_classifier_module(prev: keras.layers.Layer) -> keras.layers.Layer:\n",
    "        classifier = keras.layers.Flatten()(prev)\n",
    "        classifier = keras.layers.Dense(512, activation='relu')(classifier)\n",
    "        classifier = keras.layers.BatchNormalization()(classifier)\n",
    "        classifier = keras.layers.Dropout(.5)(classifier)\n",
    "\n",
    "        return classifier\n",
    "\n",
    "    _input = keras.layers.Input(shape=(SMALLER_HEIGHT, SMALLER_WIDTH, 3))\n",
    "    conv = get_conv_module(_input, 64, .25, 3)\n",
    "    conv = get_conv_module(conv, 128, .4, 3)\n",
    "    conv = get_conv_module(conv, 256, .5, 3)\n",
    "    attention1 = get_attention_module(conv)\n",
    "    attention2 = get_attention_module(conv)\n",
    "    merged_attentions = keras.layers.concatenate([attention1, attention2])\n",
    "    classifier = get_classifier_module(merged_attentions)\n",
    "    output = keras.layers.Dense(num_classes, activation='softmax', name='root')(classifier)\n",
    "    gap_attention1 = keras.layers.GlobalAveragePooling2D()(attention1)\n",
    "    gap_attention2 = keras.layers.GlobalAveragePooling2D()(attention2)\n",
    "    aux_output = keras.layers.Dot(axes=1, normalize=True, name='dot')([gap_attention1, gap_attention2])\n",
    "    model = keras.Model(_input, outputs=[output, aux_output])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
